# ddqn-kami2

### ddqn-kami2是什么，项目目的是什么
kami2中文名是神折纸2，是一款带点艺术性的益智折纸游戏，我过去很喜欢玩。游戏规则：如下图所示，使用下方色块，将图中颜色变为一种颜色。    
dqn（deep Q-network）是一种融合了神经网络和Q-learning的深度强化学习算法，我最初选择的就是dqn。   
ddqn(double deep Q-network)则是解决dqn过高估计问题的改进版本。   
所以项目目的就是使用ddqn训练模型来玩神折纸2，达到较高的游戏水平。在这个项目里，我搭建了能完全控制的游戏环境。  
<center>
<img src="./level_image/14_4.png" width="40%">  
游戏截图
</center>

### 项目结构
- config -------> 存放不同关卡的游戏状态文件   
- level_image -------> 从模拟器截出来的游戏截图，通过图片处理后得到config中的文件   
- closed_color.py -------> 给出一种颜色和颜色集，从颜色集中返回与该颜色最接近的颜色，游戏截图中的三角块并非纯色，所以需要获取最接近的颜色  
- game_logic.py -------> 游戏环境搭建以及奖励函数设置   
- image_process.py -------> 图片处理，将level_image的图片转化为config的状态文件   
- net.py -------> 神经网络结构文件   
- train.py -------> 训练神经网络模型   
- tran_onnx_model.py -------> 将训练好的模型转化为onnx格式，之后单独使用模型时可以提高运行效率, 减小需要导入的库   

### 如何运行
你可以创建一个干净的python虚拟环境(适用于python3.3以上版本)   
`python -m venv $虚拟环境目录`   
激活虚拟环境   
linux环境：  
`$虚拟环境目录/bin/activate`   
windwos环境：  
`cd $虚拟环境目录/`  
`Scripts/activate`  
在虚拟环境中切换到项目目录下  
`cd $项目目录`  
安装依赖   
(pytorch的版本请根据自身电脑的gpu选择不同版本，参考https://pytorch.org/get-started/locally/)   
`pip install -r requirements.txt`   
运行训练文件   
`python train.py`   

### 游戏环境处理
如上图所示，细数可以看到游戏界面总共有20列，前4行分别有15、14、14、15个三角块，之后的列同理。   
根据底部的颜色种类n，使用大小为290的数组保存状态，其每一项值取值范围为0到n-1。  

### 遇到的问题
事实上，我遇到的这些问题（不止下面的问题），并不是一个一个出现，而是同时出现的，所以这个解决问题的过程中是相当挫败的，不断修改，可是模型一直没有收敛，直到把所有问题都解决了。  
#### dqn训练模型时loss为什么会不断增大呢？  
- dqn过估计： [DQN与DDQN](https://zhuanlan.zhihu.com/p/576414326)    
- 优化器的选择：我一开始使用的是sgd，但是对于新手参数很难调合适，使用了adam优化器后解决了这个问题。  

#### 在游戏过程中，不是所有动作都是合法的，比如使用同一种颜色点击相应的三角块，或者使用已经被消除的颜色点击三角块，这些动作会导致游戏进入循环，使训练卡住，无法继续进行。采用什么方式能裁剪非法动作呢？  
[在DRL中，假如并不是所有动作都可以选择，那么对于动作的约束应该加到哪里呢？](http://www.deeprlhub.com/d/284-drl)  
最终我没有采用上面方法，比如添加mask层，或者将非法动作的奖励调到很大的负值，我使用的方式是将非法动作映射为合法动作。


### 学习深度强化学习的一点小建议
正如王树森所言，理解深度强化学习中的概念非常重要，像奖励reward和回报return的区分。在深度强化学习中，有不少概念需要理解，这部分需要花比较多的时间。   
我在写这个项目的时候，有大概半个月的时间，一直看各种教程和视频，就是因为不知道得到最终回报后怎么获取每一步的奖励，最终恍然大悟，要我自己先定义每一步得到的奖励，再去获取最终回报。虽然浪费了很多时间，但是这个过程中也对dqn和各个概念有更深入的理解。  
然后你也可以像我这样，从喜欢的游戏出发，自己搭建环境，在这个过程中去理解各个概念，解决遇到的问题。   
其次是数学基础，如果基础比较薄弱，像我，如果想更进一步，还是要去补像微积分这些的基础。   
### 参考资料
王树森的课程讲的特别好，很清晰, 看完前四章对于dqn的概念就很清晰了：  
[教程pdf](https://github.com/wangshusen/DRL/blob/master/Notes_CN/DRL.pdf)  
[视频教程](https://www.bilibili.com/video/BV12o4y197US)  


[pytorch 深度强化学习教程](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)

错误排查：   
[DQN与DDQN](https://zhuanlan.zhihu.com/p/576414326)    
[在DRL中，假如并不是所有动作都可以选择，那么对于动作的约束应该加到哪里呢？](http://www.deeprlhub.com/d/284-drl)   

